{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "###Â useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def print_latex_line(model_name, results_dict):\n",
    "    ordered_datasets = ['SimVerb3500-dev', 'MEN-dev', 'SimLex999', 'SimLex333', 'SimVerb3500-test', 'RG65', 'SCWS','MEN-test', 'MTurk', 'WS353'] \n",
    "    print \" & \".join([model_name] + [\"{0:0.1f}\".format(results_dict[e]*100) for e in ordered_datasets]) + \"\\\\\\\\\"\n",
    "    \n",
    "def csv_to_dict(fname, datasets, csv_sep=\",\", weights=[]):\n",
    "    res = {}\n",
    "    with open(fname, \"r\") as f:\n",
    "        measure_names = f.readline().rstrip().split(csv_sep)\n",
    "        measure_vals = f.readline().rstrip().split(csv_sep)\n",
    "        measure_vals = [float(v) for v in measure_vals]\n",
    "    scores = []\n",
    "    for n, v in zip(measure_names, measure_vals):\n",
    "        if not n or n not in datasets:\n",
    "            continue\n",
    "        res[n] = v\n",
    "        scores.append(v)\n",
    "    if sum(scores) > 0:\n",
    "        if len(weights) == len(scores):\n",
    "            res['mean'] = sum([res[datasets[i]] * w for i, w in enumerate(weights)])\n",
    "        else:\n",
    "            res['mean'] = sum(scores) / float(len(scores))\n",
    "    else:\n",
    "        res['mean'] = np.nan\n",
    "    return res\n",
    "\n",
    "\n",
    "def store_eval(embedding_root_dir, datasets, prefix = \"\", weights=[]):\n",
    "    \"\"\"\n",
    "    store all results located in embedding_root_dir in a dictionary\n",
    "    and adds an optional prefix at the key\n",
    "    then returns the dict\n",
    "    \"\"\"\n",
    "    results_dict = dict()\n",
    "    emb_dirs = glob.glob(embedding_root_dir + \"/*\")\n",
    "    for _dir in emb_dirs:\n",
    "        eval_fname = _dir + \"/eval\"\n",
    "        model_name = _dir.split(\"/\")[-1]\n",
    "\n",
    "        if not os.path.isfile(eval_fname):\n",
    "            continue\n",
    "        full_name = prefix+model_name\n",
    "        if full_name in results_dict:\n",
    "            raise ValueError()\n",
    "        results_dict[prefix+model_name] = csv_to_dict(eval_fname, datasets, weights=weights)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on full dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full results\n"
     ]
    }
   ],
   "source": [
    "emb_root = \"../embeddings/en_wn_full/\"\n",
    "emb_root_wn_retrofitting = \"../embeddings/en_wn_full_retrofitting/\"\n",
    "\n",
    "test_sets=['SimVerb3500-dev', 'MEN-dev','MEN-test', 'SimVerb3500-test', 'MEN-test', 'WS353', 'RW', 'RG65', 'SimLex999', 'MTurk', 'SCWS', 'SimLex333']\n",
    "dev_sets=['SimVerb3500-dev', 'MEN-dev']\n",
    "weights=[1,0.5] # weights of importance for dev_sets\n",
    "embedding_eval = \"../\"\n",
    "\n",
    "results_dict = {}\n",
    "results_dict.update(store_eval(emb_root, dev_sets, \"\", weights))\n",
    "results_dict.update(store_eval(emb_root_wn_retrofitting, dev_sets, \"\", weights))\n",
    "\n",
    "models = results_dict.keys()\n",
    "mean_scores = [results_dict[m]['mean'] for m in models]\n",
    "idx_sorted = np.argsort(mean_scores)\n",
    "\n",
    "verbose=True\n",
    "best_dict = None\n",
    "best_hill = None\n",
    "best_ae = None\n",
    "best_retrofitting = None\n",
    "best_dict_notw2v = None\n",
    "\n",
    "for i in idx_sorted:\n",
    "    if verbose:\n",
    "        print models[i], \":\", mean_scores[i]\n",
    "    if np.isnan(mean_scores[i]):\n",
    "        continue\n",
    "    if 'pen' in models[i]:\n",
    "        best_dict = models[i]\n",
    "    if 'pen' in models[i] and not 'w2v' in models[i]:\n",
    "        best_dict_notw2v = models[i]\n",
    "    if 'hill' in models[i]:\n",
    "        best_hill = models[i]\n",
    "    if 'pen0' in models[i]:\n",
    "        best_ae = models[i]\n",
    "    if 'retrofit' in models[i]:\n",
    "        best_retrofitting = models[i]\n",
    "    if verbose:\n",
    "        try:\n",
    "            print \" ; \".join([dataset + \" : \" + str(results_dict[models[i]][dataset]) for dataset in dev_sets])\n",
    "        except:\n",
    "            print \n",
    "            \n",
    "print \"\"\n",
    "print \"Full results\"\n",
    "if best_dict:\n",
    "    print \"Best CPAE w/ pretrained\"\n",
    "    print best_dict\n",
    "    d = csv_to_dict(emb_root + \"/\" + best_dict + \"/eval\", test_sets)\n",
    "    print d\n",
    "    print_latex_line(\"CPAE\", d)\n",
    "if best_dict_notw2v:\n",
    "    print \"Best CPAE w/ pretrained not w2v\"\n",
    "    print best_dict_notw2v\n",
    "    d = csv_to_dict(emb_root + \"/\" + best_dict_notw2v + \"/eval\", test_sets)\n",
    "    print d\n",
    "    print_latex_line(\"CPAE notw2v\", d)\n",
    "if best_ae:\n",
    "    print \"Best AE w/ pretrained\"\n",
    "    print best_ae\n",
    "    d = csv_to_dict(emb_root + \"/\" + best_ae + \"/eval\", test_sets)\n",
    "    print d\n",
    "    print_latex_line(\"AE\", d)\n",
    "if best_hill:\n",
    "    print \"Best Hill w/ pretrained\"\n",
    "    print best_hill\n",
    "    d = csv_to_dict(emb_root + \"/\" + best_hill + \"/eval\", test_sets)\n",
    "    print d\n",
    "    print_latex_line(\"Hill\", d)\n",
    "if best_hill:\n",
    "    print \"Best retrofitting\"\n",
    "    print best_retrofitting\n",
    "    d = csv_to_dict(emb_root_wn_retrofitting + \"/\" + best_retrofitting + \"/eval\", test_sets)\n",
    "    print d\n",
    "    print_latex_line(\"retrofitting\", d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
